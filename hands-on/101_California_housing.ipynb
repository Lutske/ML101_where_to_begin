{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "tLSSv5W_b7IT"
      },
      "source": [
        "# Machine Learning 101: How to start?\n",
        "\n",
        "So let's get started with some hands on machine learning!\n",
        "\n",
        "\n",
        "First thing first. Let's install some packages. Select the next code block and press enter to execute.\n",
        "\n",
        "We are going to use python. Most of the code block will be pre-filled. So no worries if you're not an python expert. When you have any questions. Don't hesitate to ask!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Importing Libraries"
      ],
      "metadata": {
        "id": "LUt8imdsFcqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn3lfxLxb7IV"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy matplotlib scikit-learn jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Zyx6p1b7IW"
      },
      "source": [
        "Once you've installed these packages, you're ready to start working with machine learning in Jupyter Notebook.\n",
        "\n",
        "Let's start by importing the packages we'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnCwecTOb7IX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l87cgMzrb7IX"
      },
      "source": [
        "### Step 2: Loading the dataset\n",
        "\n",
        "Next, let's load in a dataset. We will start with some text driven data. For this example, we'll use the California Housing dataset from scikit-learn. \n",
        "\n",
        "You can load the dataset using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl8ZH6NJb7IY"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california = fetch_california_housing(as_frame=True)\n",
        "dataFrame = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "dataFrame['prices'] = california.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Explorartory Data Analysis (EDA)\n",
        "\n",
        "Before diving into machine learning, it's important to understand the data.\n",
        "Let's take a look at the first few rows of the dataset:"
      ],
      "metadata": {
        "id": "pgx0SKGKF0HF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AporHMcFb7IY"
      },
      "outputs": [],
      "source": [
        "dataFrame.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UKYDLyob7IZ"
      },
      "source": [
        "Some datasets come with a description. Let's get the description of this dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(california.DESCR)"
      ],
      "metadata": {
        "id": "Q3wYkdngGT5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want some more information about the statistics on this dataframe. \n",
        "\n",
        "By calling `.describe()`, you get a summary table that provides an overview of the distribution and statistical properties of your dataset's numerical columns. It is useful for quickly understanding the range, spread, and central tendency of the data, as well as identifying potential outliers or unusual patterns."
      ],
      "metadata": {
        "id": "dY1jvDNQG_IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.describe()"
      ],
      "metadata": {
        "id": "CPs3fhaYGv9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the columns\n",
        "\n",
        "*   count: The number of non-missing values in the column.\n",
        "*   mean: The average value of the column.\n",
        "*   std: The standard deviation of the column, which measures the spread or dispersion of the values.\n",
        "*   min: The minimum value in the column.\n",
        "*   25%, 50%, 75%: The quartiles of the column. The 25th percentile (1st quartile) represents the value below which 25% of the data falls, the 50th percentile (2nd quartile) represents the median, and the 75th percentile (3rd quartile) represents the value below which 75% of the data falls.\n",
        "*   max: The maximum value in the column."
      ],
      "metadata": {
        "id": "EjyBDzw-Htwi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idVVxgcmb7IZ"
      },
      "source": [
        "### Step 4: Preparing the data\n",
        "Before training a machine learning model, we need to prepare the data by splitting it into input features `(mostly called X, here housing)` and target variable `(mostly called y, here prices)`.\n",
        "\n",
        "Now let's split the data into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVvYIAHfb7IZ"
      },
      "outputs": [],
      "source": [
        "housing = dataFrame.drop('prices', axis=1)\n",
        "pricing = dataFrame['prices']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Splitting the Data\n",
        "To evaluate the performance of our model, we'll split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance on unseen data.\n"
      ],
      "metadata": {
        "id": "3jCzuSZ8JCab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_train, housing_test, pricing_train, pricing_test = train_test_split(housing, pricing, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7ECKUnigJI4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the dataset housing and pricing will be split into training and testing sets, where 20% of the data will be reserved for testing (test_size=0.2). The random_state is set to 42, ensuring that the data is shuffled and split in a consistent manner across different runs of the code.\n",
        "\n",
        "Using a fixed random_state value helps in achieving reproducibility, as the same split will be generated each time the code is executed with the same random_state value. This is beneficial for sharing and comparing results, especially when fine-tuning models or conducting experiments."
      ],
      "metadata": {
        "id": "5_RLIq0CJl4l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnfCTr02b7Ia"
      },
      "source": [
        "We will also need to scale our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqkNO4vEb7Ia"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "housing_train_scaled = scaler.fit_transform(housing_train)\n",
        "housing_test_scaled = scaler.transform(housing_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uPrlOqnb7Ia"
      },
      "source": [
        "### Step 6: Training the model\n",
        "Now we can create our linear regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2kkIljVb7Ib"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "model.fit(housing_train_scaled, pricing_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0anRZlb7Ib"
      },
      "source": [
        "### Step 7: Making Predictions\n",
        "Once the model is trained, we can use it to make predictions on new data. Let's make predictions on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgVJeUjNb7Ib"
      },
      "outputs": [],
      "source": [
        "pricing_pred = model.predict(housing_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7YP68Rcb7Ic"
      },
      "source": [
        "### Step 8: Evaluating the Model \n",
        "Finally, let's evaluate our model using the mean squared error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGBEqkeGb7Ic"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(pricing_test, pricing_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The meaning om Mean squared error**  \n",
        "\n",
        "The mean squared error (MSE) is a commonly used metric to evaluate the performance of regression models. It measures the average squared difference between the predicted values and the actual values. The formula for calculating MSE is:\n",
        "\n",
        "MSE = (1/n) * Σ(yᵢ - ȳ)²\n",
        "\n",
        "where:\n",
        "\n",
        "*   n is the number of samples in the dataset\n",
        "*   yᵢ is the predicted value for the i-th sample\n",
        "*   ȳ is the actual (true) value for the i-th sample\n",
        "\n",
        "\n",
        "MSE provides a measure of how close the predicted values are to the true values on average. It calculates the average squared deviation, which means that larger deviations from the true values are penalized more.\n",
        "\n",
        "A lower value of MSE indicates better performance, as it means that the predicted values are closer to the true values. Conversely, a higher value of MSE indicates larger errors and greater discrepancy between the predicted and actual values.\n",
        "\n",
        "It's important to note that the interpretation of \"good\" or \"bad\" MSE values depends on the specific context of the problem. The scale of the MSE depends on the units of the target variable. For example, if the target variable represents house prices in thousands of dollars, an MSE of 10,000 would mean an average squared error of $10,000,000, which might be considered high. However, it's crucial to compare the MSE with other models or establish a baseline to determine the relative performance.\n",
        "\n",
        "In summary, a lower MSE indicates better performance, but the interpretation of what constitutes a \"good\" MSE value depends on the specific context and should be considered relative to other models or benchmarks."
      ],
      "metadata": {
        "id": "_HyYiam-9cYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Visualize the predictions\n",
        "Finally, let's visualize the predicted values compared to the actual values."
      ],
      "metadata": {
        "id": "_h1MnZxPK5TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(pricing_test, pricing_pred)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Au66xQ5d7AN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7WKBS5ab7Ic"
      },
      "source": [
        "And that's it! You should now have a basic understanding of how to work with the California Housing dataset and build a machine learning model to predict housing prices."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}