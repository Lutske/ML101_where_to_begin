{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "tLSSv5W_b7IT"
      },
      "source": [
        "# Machine Learning 101: How to start?\n",
        "\n",
        "So let's get started with some hands on machine learning!\n",
        "\n",
        "\n",
        "First thing first. Let's install some packages. Select the next code block and press enter to execute.\n",
        "\n",
        "We are going to use python. Most of the code block will be pre-filled. So no worries if you're not an python expert. When you have any questions. Don't hesitate to ask!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Importing Libraries"
      ],
      "metadata": {
        "id": "LUt8imdsFcqG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn3lfxLxb7IV"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy matplotlib scikit-learn jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0Zyx6p1b7IW"
      },
      "source": [
        "Once you've installed these packages, you're ready to start working with machine learning in Jupyter Notebook.\n",
        "\n",
        "Let's start by importing the packages we'll be using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnCwecTOb7IX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l87cgMzrb7IX"
      },
      "source": [
        "### Step 2: Loading the dataset\n",
        "\n",
        "Next, let's load in a dataset. We will start with numerical and categorical data. For this example, we'll use the California Housing dataset from scikit-learn.\n",
        "\n",
        "You can load the dataset using the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl8ZH6NJb7IY"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california = fetch_california_housing(as_frame=True)\n",
        "dataFrame = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "dataFrame['prices'] = california.target"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Explorartory Data Analysis (EDA)\n",
        "\n",
        "Before diving into machine learning, it's important to understand the data.\n",
        "Let's take a look at the first few rows of the dataset:"
      ],
      "metadata": {
        "id": "pgx0SKGKF0HF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AporHMcFb7IY"
      },
      "outputs": [],
      "source": [
        "dataFrame.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UKYDLyob7IZ"
      },
      "source": [
        "Some datasets come with a description. Let's get the description of this dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(california.DESCR)"
      ],
      "metadata": {
        "id": "Q3wYkdngGT5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want some more information about the statistics on this dataframe.\n",
        "\n",
        "By calling `.describe()`, you get a summary table that provides an overview of the distribution and statistical properties of your dataset's numerical columns. It is useful for quickly understanding the range, spread, and central tendency of the data, as well as identifying potential outliers or unusual patterns."
      ],
      "metadata": {
        "id": "dY1jvDNQG_IL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.describe()"
      ],
      "metadata": {
        "id": "CPs3fhaYGv9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the columns\n",
        "\n",
        "*   count: The number of non-missing values in the column.\n",
        "*   mean: The average value of the column.\n",
        "*   std: The standard deviation of the column, which measures the spread or dispersion of the values.\n",
        "*   min: The minimum value in the column.\n",
        "*   25%, 50%, 75%: The quartiles of the column. The 25th percentile (1st quartile) represents the value below which 25% of the data falls, the 50th percentile (2nd quartile) represents the median, and the 75th percentile (3rd quartile) represents the value below which 75% of the data falls.\n",
        "*   max: The maximum value in the column."
      ],
      "metadata": {
        "id": "EjyBDzw-Htwi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idVVxgcmb7IZ"
      },
      "source": [
        "### Step 4: Preparing the data\n",
        "Before training a machine learning model, we need to prepare the data by splitting it into input features `(mostly called X, here housing)` and target variable `(mostly called y, here prices)`.\n",
        "\n",
        "Now let's split the data into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVvYIAHfb7IZ"
      },
      "outputs": [],
      "source": [
        "housing = dataFrame.drop('prices', axis=1)\n",
        "pricing = dataFrame['prices']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Splitting the Data\n",
        "To evaluate the performance of our model, we'll split the data into training and testing sets. The training set will be used to train the model, while the testing set will be used to evaluate its performance on unseen data.\n"
      ],
      "metadata": {
        "id": "3jCzuSZ8JCab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_train, housing_test, pricing_train, pricing_test = train_test_split(housing, pricing, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7ECKUnigJI4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the dataset housing and pricing will be split into training and testing sets, where 20% of the data will be reserved for testing (test_size=0.2). The random_state is set to 42, ensuring that the data is shuffled and split in a consistent manner across different runs of the code.\n",
        "\n",
        "Using a fixed random_state value helps in achieving reproducibility, as the same split will be generated each time the code is executed with the same random_state value. This is beneficial for sharing and comparing results, especially when fine-tuning models or conducting experiments."
      ],
      "metadata": {
        "id": "5_RLIq0CJl4l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnfCTr02b7Ia"
      },
      "source": [
        "We will also need to scale our data.\n",
        "\n",
        "Check if the features in your dataset are on different scales. If so, consider applying feature scaling techniques such as standardization or normalization to bring all features to a similar scale. This can help the model converge faster and improve its performance. `StandardScaler()` in transforms the data by scaling it to have zero mean and unit variance, ensuring all features are on a similar scale.\n",
        "\n",
        "We use `fit_transform()` on the training data during the preprocessing step to learn the necessary transformations and apply them to the data. This allows the model to learn and adapt to the training data's characteristics.\n",
        "\n",
        "On the other hand, we use `transform()` on the testing data to apply the same learned transformations without relearning them, ensuring consistency between the training and testing datasets and enabling fair evaluation of the model's performance on unseen data. This way, the testing data undergoes the same preprocessing steps as the training data, maintaining the integrity of the feature scaling or other transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqkNO4vEb7Ia"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "housing_train_scaled = scaler.fit_transform(housing_train)\n",
        "housing_test_scaled = scaler.transform(housing_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uPrlOqnb7Ia"
      },
      "source": [
        "### Step 6: Training the model\n",
        "Now we can create our linear regression model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2kkIljVb7Ib"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "model.fit(housing_train_scaled, pricing_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0anRZlb7Ib"
      },
      "source": [
        "### Step 7: Making Predictions\n",
        "Once the model is trained, we can use it to make predictions on new data. Let's make predictions on the testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgVJeUjNb7Ib"
      },
      "outputs": [],
      "source": [
        "pricing_pred = model.predict(housing_test_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7YP68Rcb7Ic"
      },
      "source": [
        "### Step 8: Evaluating the Model\n",
        "Finally, let's evaluate our model using the mean squared error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGBEqkeGb7Ic"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(pricing_test, pricing_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The meaning om Mean squared error**\n",
        "\n",
        "The mean squared error (MSE) is a commonly used metric to evaluate the performance of regression models. It measures the average squared difference between the predicted values and the actual values. The formula for calculating MSE is:\n",
        "\n",
        "MSE = (1/n) * Σ(yᵢ - ȳ)²\n",
        "\n",
        "where:\n",
        "\n",
        "*   n is the number of samples in the dataset\n",
        "*   yᵢ is the predicted value for the i-th sample\n",
        "*   ȳ is the actual (true) value for the i-th sample\n",
        "\n",
        "\n",
        "MSE provides a measure of how close the predicted values are to the true values on average. It calculates the average squared deviation, which means that larger deviations from the true values are penalized more.\n",
        "\n",
        "A lower value of MSE indicates better performance, as it means that the predicted values are closer to the true values. Conversely, a higher value of MSE indicates larger errors and greater discrepancy between the predicted and actual values.\n",
        "\n",
        "It's important to note that the interpretation of \"good\" or \"bad\" MSE values depends on the specific context of the problem. The scale of the MSE depends on the units of the target variable. For example, if the target variable represents house prices in thousands of dollars, an MSE of 10,000 would mean an average squared error of $10,000,000, which might be considered high. However, it's crucial to compare the MSE with other models or establish a baseline to determine the relative performance.\n",
        "\n",
        "In summary, a lower MSE indicates better performance, but the interpretation of what constitutes a \"good\" MSE value depends on the specific context and should be considered relative to other models or benchmarks."
      ],
      "metadata": {
        "id": "_HyYiam-9cYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Visualize the predictions\n",
        "Finally, let's visualize the predicted values compared to the actual values."
      ],
      "metadata": {
        "id": "_h1MnZxPK5TE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(pricing_test, pricing_pred)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Values')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Au66xQ5d7AN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7WKBS5ab7Ic"
      },
      "source": [
        "Congrats, the basic model is in place! You should now have a basic understanding of how to work with the California Housing dataset and build a machine learning model to predict housing prices."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving the model\n",
        "Now that we have a basic model in place, let's explore ways to improve its\n",
        "performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "2yBquXiBN771"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Feature Engineering\n",
        "Consider exploring the dataset and creating additional features based on domain knowledge or feature interactions. This can provide the model with more relevant input and potentially improve its predictive power.\n",
        "\n",
        "⚠ This can also make your model much worse. Take the next in mind:\n",
        "\n",
        "1.   Cost-benefit tradeoff: Feature engineering may not offer substantial gains relative to the effort invested.\n",
        "2.   Feature sufficiency: Existing features might already provide enough information for accurate predictions, reducing the need for further engineering.\n",
        "3.   Overfitting risk: Care must be taken to avoid overfitting when introducing new features, as it can lead to poor generalization and performance degradation on unseen data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "INrWsXhsP1nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Creating interaction terms\n",
        "dataFrame['rooms_per_person'] = dataFrame['total_rooms'] / dataFrame['population']"
      ],
      "metadata": {
        "id": "u8t_NHqYOWgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Hyperparameter Tuning\n",
        "Experiment with different hyperparameter values for the linear regression model. Hyperparameters control the behavior of the model and can significantly impact its performance. Use techniques like grid search or random search to find the best combination of hyperparameter values.\n",
        "\n"
      ],
      "metadata": {
        "id": "RO8Vo_x9OjTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Grid Search with Cross-Validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
        "model = Ridge()\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(housing_train_scaled, pricing_train)\n",
        "\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "JbxeKO9BPWLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Model Selection\n",
        "Try different algorithms or models and compare their performance. Linear regression is just one algorithm for regression tasks. Explore other algorithms such as decision trees, random forests, support vector regression, or gradient boosting. Each algorithm has its own strengths and weaknesses, and it's worth experimenting with different models to find the best one for your problem.\n"
      ],
      "metadata": {
        "id": "rep7aOLcOvQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Random Forest Regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "model.fit(housing_train_scaled, pricing_train)\n",
        "pricing_pred = model.predict(housing_test_scaled)\n",
        "mse = mean_squared_error(pricing_test, pricing_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Example 2: Gradient Boosting Regression (GBR)\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "model = GradientBoostingRegressor()\n",
        "model.fit(housing_train_scaled, pricing_train)\n",
        "pricing_pred = model.predict(housing_test_scaled)\n",
        "mse = mean_squared_error(pricing_test, pricing_pred)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "\n",
        "# Try with a different model like  Support Vector Regression (SVR)\n",
        "from sklearn.svm import SVR"
      ],
      "metadata": {
        "id": "bi0SPBjPPWkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Handling Outliers\n",
        "Investigate if there are any outliers in your dataset and consider how to handle them. Outliers can have a significant impact on the model's performance. Depending on the situation, you might choose to remove outliers, apply transformations, or use robust models that are less sensitive to outliers."
      ],
      "metadata": {
        "id": "vGrzeD7GO3B-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Winsorization\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "dataFrame['prices'] = winsorize(dataFrame['prices'], limits=[0.05, 0.05])\n",
        "# Try to find out the correct limits for this dataFrame."
      ],
      "metadata": {
        "id": "hjNZSABlQAF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example the limits set at the 5th and 95th percentiles This means that values below the 5th percentile are replaced with the value at the 5th percentile, and values above the 95th percentile are replaced with the value at the 95th percentile."
      ],
      "metadata": {
        "id": "La1a-wEq--tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Cross-Validation\n",
        "Instead of relying on a single train-test split, consider using cross-validation techniques to evaluate the model's performance. Cross-validation provides a more robust estimate of the model's performance by splitting the data into multiple train-test splits and evaluating the model on each split.\n"
      ],
      "metadata": {
        "id": "bLwE92YUPDgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: K-Fold Cross-Validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "model = LinearRegression()\n",
        "scores = cross_val_score(model, housing, prices, cv=5, scoring='neg_mean_squared_error')\n",
        "mse_scores = -scores"
      ],
      "metadata": {
        "id": "bpWM-DwuQGiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When cv is set to 5, it means that the dataset will be split into 5 equal-sized folds or subsets. The cross-validation process will then be performed 5 times, where each time, one of the folds will be used as the validation set, and the remaining 4 folds will be used as the training set. This allows for comprehensive evaluation of the model's performance by rotating the validation set across different portions of the data."
      ],
      "metadata": {
        "id": "MkxBhU0cAEwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Collect More Data\n",
        "If feasible, collecting more data can often help improve the model's performance. More data provides the model with more examples to learn from, reduces overfitting, and helps capture the underlying patterns in the data more effectively.\n"
      ],
      "metadata": {
        "id": "fg26X8TdPGet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Regularization Techniques\n",
        "Consider applying regularization techniques such as L1 or L2 regularization to the linear regression model. Regularization helps prevent overfitting by adding a penalty term to the model's loss function. It encourages the model to have simpler and smoother solutions, reducing the risk of overfitting the training data.\n"
      ],
      "metadata": {
        "id": "71E2NTE_PJtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Lasso Regression\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(housing_train_scaled, pricing_train)"
      ],
      "metadata": {
        "id": "aOrlwXiuQKcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of Lasso regression, the `alpha` parameter controls the strength of the regularization penalty applied to the model. A value of `alpha=0.1` means that a moderate regularization penalty will be applied during the training of the Lasso regression model.\n",
        "\n",
        "Lasso regression includes a term in the loss function that adds the sum of the absolute values of the coefficients multiplied by alpha to the ordinary least squares loss. This penalty encourages the model to shrink the coefficients towards zero, promoting sparsity and feature selection.\n",
        "\n",
        "By setting alpha=0.1, you are specifying a moderate amount of regularization. Increasing the value of alpha towards 1 will increase the strength of the regularization, potentially resulting in more coefficients being set to zero and a more sparse model. Conversely, reducing the value of alpha towards 0 will decrease the regularization effect, allowing the model to fit the data more closely. The choice of the alpha parameter depends on the specific dataset and the trade-off between bias and variance in the model."
      ],
      "metadata": {
        "id": "nFmGQK3FAiCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusion\n",
        "Remember that improving the model's performance is an iterative process. It requires experimentation, analysis of results, and fine-tuning of various components. Iterate on these steps, try different approaches, and carefully evaluate the impact on the model's performance.\n",
        "\n",
        "Congratulations on completing this Machine Learning 101 tutorial! You've learned the fundamental steps of building and evaluating a machine learning model using Python and various libraries. Continue to explore and expand your knowledge in the exciting field of machine learning."
      ],
      "metadata": {
        "id": "rYSGZTWBPM1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bonus\n",
        "\n",
        "Edit the above code to make the mean squared error smaller."
      ],
      "metadata": {
        "id": "ISxsw_QlQQOM"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}