{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Choose Your Own Dataset and Model\n",
        "\n",
        "Let's create your own machine learning model!\n",
        "\n",
        "Start by searching for a dataset on the internet. You can find datasets at:\n",
        "\n",
        "* [Kaggle Datasets](www.kaggle.com/datasets),\n",
        "* [UCI Machine Learning Repository](www.archive.ics.uci.edu/ml/datasets.php),\n",
        "* [Google Dataset Search](www.datasetsearch.research.google.com),\n",
        "* [OpenML](www.openml.org/search?sort=date),\n",
        "* [Data.gov](www.data.gov),\n",
        "* [Data Overheid NL](https://www.data.overheid.nl),\n",
        ""
      ],
      "metadata": {
        "id": "H5KV5Y3te0fN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 1: Importing Libraries"
      ],
      "metadata": {
        "id": "NDH_5RkAg6e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you can also choose to edit the libraries\n",
        "pip install pandas numpy matplotlib scikit-learn jupyter"
      ],
      "metadata": {
        "id": "ViRl6nfRdpwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Loading the dataset"
      ],
      "metadata": {
        "id": "9GDXQ7QLhBXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Example: Load the \"Adoptable Pets\" dataset from Data.gov (https://catalog.data.gov/dataset/adoptable-pets)\n",
        "#from sklearn.datasets import fetch_openml\n",
        "\n",
        "#adoptable_pets = fetch_openml(data_id=36029, as_frame=True)\n",
        "\n",
        "#[Import your needed libraries]\n",
        "#[Load your dataset]"
      ],
      "metadata": {
        "id": "p5mq8IjchGVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Explorartory Data Analysis (EDA)\n",
        "Understand the data you are using"
      ],
      "metadata": {
        "id": "FTOdVUSJhx7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.head()"
      ],
      "metadata": {
        "id": "K05gN93Xh3-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may also check if the dataset has a description."
      ],
      "metadata": {
        "id": "6X52VXS7h8OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(california.DESCR)"
      ],
      "metadata": {
        "id": "DYPVuN9Rh-9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the description of the data. As a reminder the explinantion of the columns is:\n",
        "\n",
        "*   count: The number of non-missing values in the column.\n",
        "*   mean: The average value of the column.\n",
        "*   std: The standard deviation of the column, which measures the spread or dispersion of the values.\n",
        "*   min: The minimum value in the column.\n",
        "*   25%, 50%, 75%: The quartiles of the column. The 25th percentile (1st quartile) represents the value below which 25% of the data falls, the 50th percentile (2nd quartile) represents the median, and the 75th percentile (3rd quartile) represents the value below which 75% of the data falls.\n",
        "*   max: The maximum value in the column.\n"
      ],
      "metadata": {
        "id": "J_tkEJ4TiC0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.describe()"
      ],
      "metadata": {
        "id": "myROZF_JiPHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Preparing the data\n",
        "Split the data into features and target variables"
      ],
      "metadata": {
        "id": "5ree61MwinyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do something with dataframe.drop if you have a labeled data"
      ],
      "metadata": {
        "id": "zdl5pwUyizV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Splitting the Data\n",
        "\n",
        "Split the data into training and testing  (and validation) sets"
      ],
      "metadata": {
        "id": "U2bAuX0Hi9Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data, you can use the method train_test_split from sklearn.model_selection"
      ],
      "metadata": {
        "id": "flWe1G04jC5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Training the model\n",
        "The exiting part"
      ],
      "metadata": {
        "id": "1HTMOQ70jwCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = # put here your model\n",
        "model.fit( # Do you remeber what should be in here? )"
      ],
      "metadata": {
        "id": "3fpke3eTjysf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Making Predictions\n",
        "\n",
        "Once the model is trained, make predictions on new data. Let's make predictions on the testing data.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQcYkVBdkEJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions = model.predict(something here)"
      ],
      "metadata": {
        "id": "ZQz17hBDkKZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Evaluating the Model\n",
        "\n",
        "Evaluate the performance of your model using metrics like Mean Squared Error."
      ],
      "metadata": {
        "id": "GwJymd_gkRo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(test_dataset, prediction_dataset)\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "Y4zFi11Hkay7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Visualize the predictions\n",
        "You can also visualize the predictions."
      ],
      "metadata": {
        "id": "VNvoARCdkvkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## example\n",
        "# plt.scatter(pricing_test, pricing_pred)\n",
        "# plt.xlabel('Actual Values')\n",
        "# plt.ylabel('Predicted Values')\n",
        "# plt.title('Actual vs. Predicted Values')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "ZBxGK7a-k3Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving the model\n",
        "\n",
        "Explore ways to optimize and improve your model."
      ],
      "metadata": {
        "id": "WPNudytdk9Zr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# More optimization code\n",
        "# for labeled data use the mean_squared_error to see if it's improving or not"
      ],
      "metadata": {
        "id": "tGAJ4SaBlBar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Additional optimization code"
      ],
      "metadata": {
        "id": "gd5YhnwMlB8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}